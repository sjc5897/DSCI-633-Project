{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell contains all the imports and needed methods\n",
    "\n",
    "\"\"\"\n",
    "DSCI-663-03 Project: Data Mining File\n",
    "This file is used for our data mining tasks, currently attempting to implement apriori association rule mining\n",
    "\n",
    ":language:      Python with pandas\n",
    ":author:        Stephen Cook\n",
    ":date created:  10/26/21\n",
    ":last edit:     11/27/21\n",
    "\"\"\"\n",
    "\n",
    "#TODO: Implement tasks 5 and 6\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "from itertools import combinations, permutations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "For our implementation of apriori, the dataframe needs to be converted into a list of records.\n",
    "Our dataset has an additional challenge in that record responses are not binary, they are catagorical\n",
    "As such, each item needs to be associated with a column in addition to it being made a list\n",
    ":param:     df: the dataframe of the file we are running\n",
    ":returns:   records:    an array of arrays, with each array representing a survey response\n",
    ":returns:   items:      an array of all the items in then records\n",
    "\"\"\"\n",
    "def init_records(df):\n",
    "    init_records = []        # init the record array\n",
    "    # iterate through the dataframe\n",
    "    for iterator in range(len(df)):\n",
    "        raw_record = df.loc[iterator]       # get the raw record at the current location of the iteration\n",
    "        record = []                         # initialize the record array\n",
    "        # iterate through each value of the raw_record\n",
    "        for column, value in raw_record.items():\n",
    "            # create a string, columnID = value and add it to the record array\n",
    "            attribute_string = str(column) + \"=\" + str(value)\n",
    "            record.append(attribute_string)\n",
    "        # add record to records\n",
    "        init_records.append(record)\n",
    "\n",
    "    # create the items array, the array of all items in the records\n",
    "    return init_records\n",
    "\"\"\"\n",
    "A general method for the implementation of the apriori algorithm used to find item sets\n",
    ":param:     iteration:  the current iteration of the apriori algorithm, also known as depth\n",
    ":param:     records:    the list of records\n",
    ":param:     min_sup:    the minimum support number\n",
    ":param:     large_set:  the last large_set of items\n",
    ":return:    count_set:  the set of all counts (Note this might not be needed)\n",
    ":return:    large_return set:   The newly created large set\n",
    "\"\"\"\n",
    "def apriori(iteration, records, min_sup, large_set):\n",
    "    large_return_set = {}  # initializes the large return set, the set of rules which pass min_sup\n",
    "    count_set = {}  # initializes the count set, the set in which all rules and counts are held\n",
    "\n",
    "    # if first iteration\n",
    "    if iteration == 0:\n",
    "        items  = sorted([item for sublist in records for item in sublist if item != 'nan'])\n",
    "        count_set = {i: items.count(i) for i in items}\n",
    "\n",
    "    # all other iterations\n",
    "    else:\n",
    "        large_set = sorted(list(large_set.keys()))  # convert the large set to a sorted list\n",
    "        larger_set = list(combinations(large_set, iteration + 1))  # generate the larger set (the next level set)\n",
    "\n",
    "        # iterate through the larger_set we created\n",
    "        for a_set in larger_set:\n",
    "            count = 0\n",
    "            # for each of the records\n",
    "            for record in records:\n",
    "                # This checks subsets\n",
    "                if set(a_set) <= set(record):\n",
    "                    # increment count\n",
    "                    count += 1\n",
    "            count_set[a_set] = count\n",
    "\n",
    "    # iterate through the count_set now\n",
    "    for key, value in count_set.items():\n",
    "        # if the count surpasses min_support_count, add it to the large_return_set\n",
    "        if value >= min_sup:\n",
    "            large_return_set[key] = value\n",
    "\n",
    "    return count_set, large_return_set\n",
    "\n",
    "\"\"\"\n",
    "prints the final set of rules along with metrics\n",
    ":param:     final_set:  The final set of rules mined\n",
    "\"\"\"\n",
    "def print_rules(final_set):\n",
    "    for rule in final_set:\n",
    "        string = rule + \"\\n\\t\" + \"Support: \" + str(final_set[rule]['support']) + \"\\n\\tConfidence: \" + str(final_set[rule]['confidence'])\n",
    "        print(string)\n",
    "\n",
    "\"\"\"\n",
    "Performs the association rule ming from all the apriori associations\n",
    ":param:     rule_set                All the rules found by apriori which are above a support level\n",
    ":param:     confidence              the confidence level inputed by the user\n",
    ":param:     total_transactions      total number of records\n",
    ":return\"    final set               the final set of rules\n",
    "\"\"\"\n",
    "def mine_association_rules(rule_set, confidence, total_transactions):\n",
    "\n",
    "    final_set = {}          # the final set of rules\n",
    "    for rule_tuple in rule_set:\n",
    "        # check if the rule is a tuple, if it isn't it is a base value and thus is ignored\n",
    "        if not isinstance(rule_tuple,tuple):\n",
    "            continue\n",
    "\n",
    "        # Calculate Support = transactions containing both X and Y/Total Transactions\n",
    "        rule_count = rule_set[rule_tuple]\n",
    "        rule_support = float(rule_count)/float(total_transactions)\n",
    "\n",
    "        # iterate through the tuples\n",
    "        for iter_1 in range(0,len(rule_tuple)+1):\n",
    "            for iter_2 in range(iter_1+1, len(rule_tuple)+1):\n",
    "\n",
    "                # Initalize A\n",
    "                A = rule_tuple[iter_1:iter_2]\n",
    "                if len(A) > 1:\n",
    "                    A = tuple(A)\n",
    "\n",
    "                # Initalize B\n",
    "                B = rule_tuple[:iter_1] + rule_tuple[iter_2:]\n",
    "                if len(B) > 1:\n",
    "                    B = tuple(B)\n",
    "\n",
    "                # Ensure Neither is len 0\n",
    "                if len(A) != 0 and len(B) != 0:\n",
    "                    # Calculate Confidence(A->B) =  (transactions containing both (A and B))/transactions containing(A)\n",
    "                    num = float(rule_set[rule_tuple])\n",
    "                    dom = float(rule_set[A[0] if len(A) == 1 else A])\n",
    "                    rule_confidence = num/dom\n",
    "                    if rule_confidence >= confidence:\n",
    "                    #     # Calculate Lift(A->B) = (transactions containing both A and B)/(Transactions containing A)/(Support(B))\n",
    "                    #     num = float(rule_set[rule_tuple])\n",
    "                    #     dom = float(rule_set[A[0] if len(A) == 1 else A])\n",
    "                    #\n",
    "                    #\n",
    "                    #     rule_lift = (float(rule_set[rule_tuple]) / float(rule_set[A[0] if len(A) == 1 else A])) / float(rule_set[B[0] if len(B) == 0 else B])\n",
    "                        rule_str = str(A if len(A) != 1 else A[0]) + ' -> ' + str(B if len(B) != 1 else B[0])\n",
    "                        final_set[rule_str] = {'support': rule_support, 'confidence': rule_confidence}\n",
    "\n",
    "    return final_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "file_name = input(\"Insert File Name: \")\n",
    "mental_health_data_frame = pd.read_csv(file_name)\n",
    "total_transactions = mental_health_data_frame.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Task 1, load the file\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "min_support_percent = input(\"Input Minimum Support Percentage (0-100): \")\n",
    "min_support_float = float(int(min_support_percent)/100)\n",
    "min_support_count = int(len(mental_health_data_frame) * min_support_float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Task 2, get minSup% and calculate the min_sup_count\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "records = init_records(mental_health_data_frame)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Task 3, convert the dataframe to a list.\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# get the number of iterations we wish to run\n",
    "# TODO: add in other metrics like confidence and lift\n",
    "iterations = int(input(\"How many iterations of Apriori do you wish to run (HIGHER VALUES REQUIRE MORE TIME AND RESOURCES LEVEL 1 IS RECOMMENDED):\"))\n",
    "\n",
    "\n",
    "large_set = {}\n",
    "rule_set = {}\n",
    "\n",
    "iteration_count = 0\n",
    "while iteration_count < iterations:\n",
    "    count_set, large_set = apriori(iteration_count,records,min_support_count,large_set)\n",
    "    rule_set.update(large_set)\n",
    "    iteration_count += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Task 4, run the apriori the user specified increments\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "confidence = float(input(\"Input the Minimum Confidence Percentage (0-100): \"))/100\n",
    "\n",
    "# MINE\n",
    "final_set = mine_association_rules(rule_set,confidence, total_transactions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Task 5, run the association rule mining to find best rules\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "print_rules(final_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Task 6, display mined rules."
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}