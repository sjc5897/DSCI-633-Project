{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell contains all the imports and needed methods\n",
    "\n",
    "\"\"\"\n",
    "DSCI-663-03 Project: Data Mining File\n",
    "This file is used for our data mining tasks, currently attempting to implement apriori association rule mining\n",
    "\n",
    ":language:      Python with pandas\n",
    ":author:        Stephen Cook\n",
    ":date created:  10/26/21\n",
    ":last edit:     11/05/21\n",
    "\"\"\"\n",
    "\n",
    "#TODO: Implement tasks 5 and 6\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "from itertools import combinations, permutations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "For our implementation of apriori, the dataframe needs to be converted into a list of records.\n",
    "Our dataset has an additional challenge in that record responses are not binary, they are catagorical\n",
    "As such, each item needs to be associated with a column in addition to it being made a list\n",
    ":param:     df: the dataframe of the file we are running\n",
    ":returns:   records:    an array of arrays, with each array representing a survey response\n",
    ":returns:   items:      an array of all the items in then records\n",
    "\"\"\"\n",
    "def init_records(df):\n",
    "    init_records = []        # init the record array\n",
    "    # iterate through the dataframe\n",
    "    for iterator in range(len(df)):\n",
    "        raw_record = df.loc[iterator]       # get the raw record at the current location of the iteration\n",
    "        record = []                         # initialize the record array\n",
    "        # iterate through each value of the raw_record\n",
    "        for column, value in raw_record.items():\n",
    "            # create a string, columnID = value and add it to the record array\n",
    "            attribute_string = str(column) + \"=\" + str(value)\n",
    "            record.append(attribute_string)\n",
    "        # add record to records\n",
    "        init_records.append(record)\n",
    "\n",
    "    # create the items array, the array of all items in the records\n",
    "    return init_records\n",
    "\"\"\"\n",
    "A general method for the implementation of the apriori algorithm used to find item sets\n",
    ":param:     iteration:  the current iteration of the apriori algorithm, also known as depth\n",
    ":param:     records:    the list of records\n",
    ":param:     min_sup:    the minimum support number\n",
    ":param:     large_set:  the last large_set of items\n",
    ":return:    count_set:  the set of all counts (Note this might not be needed)\n",
    ":return:    large_return set:   The newly created large set\n",
    "\"\"\"\n",
    "def apriori(iteration, records, min_sup, large_set):\n",
    "    large_return_set = {}  # initializes the large return set, the set of rules which pass min_sup\n",
    "    count_set = {}  # initializes the count set, the set in which all rules and counts are held\n",
    "\n",
    "    # if first iteration\n",
    "    if iteration == 0:\n",
    "        items  = sorted([item for sublist in records for item in sublist if item != 'nan'])\n",
    "        count_set = {i: items.count(i) for i in items}\n",
    "\n",
    "    # all other iterations\n",
    "    else:\n",
    "        large_set = sorted(list(large_set.keys()))  # convert the large set to a sorted list\n",
    "        larger_set = list(combinations(large_set, iteration + 1))  # generate the larger set (the next level set)\n",
    "\n",
    "        # iterate through the larger_set we created\n",
    "        for a_set in larger_set:\n",
    "            count = 0\n",
    "            # for each of the records\n",
    "            for record in records:\n",
    "                # This checks subsets\n",
    "                if set(a_set) <= set(record):\n",
    "                    # increment count\n",
    "                    count += 1\n",
    "            count_set[a_set] = count\n",
    "\n",
    "    # iterate through the count_set now\n",
    "    for key, value in count_set.items():\n",
    "        # if the count surpasses min_support_count, add it to the large_return_set\n",
    "        if value >= min_sup:\n",
    "            large_return_set[key] = value\n",
    "\n",
    "    return count_set, large_return_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "file_name = input(\"Insert File Name: \")\n",
    "mental_health_data_frame = pd.read_csv(file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Task 1, load the file\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "min_support_percent = input(\"Input Minimum Support Percentage (0-100): \")\n",
    "min_support_float = float(int(min_support_percent)/100)\n",
    "min_support_count = int(len(mental_health_data_frame) * min_support_float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Task 2, get minSup% and calculate the min_sup_count\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "records = init_records(mental_health_data_frame)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Task 3, convert the dataframe to a list.\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AA=I was aware of some': 321, 'AA=N/A (not currently aware)': 457, 'AAA=False': 488, 'AAA=True': 655, 'B=100-500': 247, 'B=26-100': 292, 'B=6-25': 209, 'B=More than 1000': 255, 'BB=None did': 681, 'BB=Some did': 229, 'BBB=Not applicable to me': 454, 'BBB=Rarely': 257, 'BBB=Sometimes': 283, 'C=False': 263, 'C=True': 880, 'CC=None did': 643, 'CC=Some did': 321, 'CCC=Not applicable to me': 383, 'CCC=Often': 421, 'CCC=Sometimes': 286, \"DD=I don't know\": 670, 'DDD=25-35': 612, 'DDD=36-45': 305, \"E=I don't know\": 319, 'E=No': 213, 'E=Yes': 528, \"EE=I don't know\": 244, 'EE=Some of them': 500, 'EE=Yes, all of them': 171, 'EEE=F': 280, 'EEE=M': 845, 'F=I am not sure': 351, 'F=No': 352, 'F=Yes': 307, 'FF=None of them': 453, 'FF=Some of them': 497, 'FFF=United States of America': 707, 'G=No': 812, 'G=Yes': 229, 'GG=No, at none of my previous employers': 359, 'GG=Some of my previous employers': 578, \"H=I don't know\": 318, 'H=No': 531, 'H=Yes': 294, 'HH=No, at none of my previous employers': 344, 'HH=Some of my previous employers': 513, 'HHH=United States of America': 714, \"I=I don't know\": 740, 'I=Yes': 319, \"II=I don't know\": 259, 'II=None did': 363, 'II=Some did': 352, 'J=Neither easy nor difficult': 178, 'J=Somewhat difficult': 199, 'J=Somewhat easy': 279, 'J=Very easy': 220, 'JJ=None of them': 631, 'JJ=Some of them': 336, 'K=Maybe': 486, 'K=No': 436, 'K=Yes': 221, 'KK=Maybe': 482, 'KK=No': 371, 'KK=Yes': 290, 'L=Maybe': 268, 'L=No': 834, 'LLL=Always': 217, 'LLL=Never': 318, 'LLL=Sometimes': 608, 'M=Maybe': 477, 'M=No': 392, 'M=Yes': 274, 'MM=Maybe': 331, 'MM=No': 720, 'N=Maybe': 381, 'N=No': 336, 'N=Yes': 426, \"O=I don't know\": 491, 'O=No': 303, 'O=Yes': 349, 'OO=Maybe': 469, 'OO=Yes, I think it would': 447, 'P=No': 1045, 'PP=Maybe': 461, \"PP=No, I don't think they would\": 300, 'PP=Yes, I think they would': 308, 'QQ=Somewhat not open': 177, 'QQ=Somewhat open': 512, 'QQ=Very open': 198, 'RR=Maybe/Not sure': 276, 'RR=No': 491, 'RR=Yes, I observed': 193, 'SS=No': 198, 'SS=No Entry': 634, 'SS=Yes': 180, \"TT=I don't know\": 213, 'TT=No': 391, 'TT=Yes': 539, 'UU=Maybe': 184, 'UU=No': 378, 'UU=Yes': 581, 'VV=Maybe': 253, 'VV=No': 440, 'VV=Yes': 450, 'Y=True': 1012, 'YY=No': 578, 'YY=Yes': 565, \"Z=I don't know\": 258, 'Z=No, none did': 280, 'Z=Some did': 323}\n"
     ]
    }
   ],
   "source": [
    "# get the number of iterations we wish to run\n",
    "# TODO: add in other metrics like confidence and lift\n",
    "iterations = int(input(\"How many iterations of Apriori do you wish to run:\"))\n",
    "large_set = {}\n",
    "iteration_count = 0\n",
    "while iteration_count < iterations:\n",
    "    count_set, large_set = apriori(iteration_count,records,min_support_count,large_set)\n",
    "    print(large_set)\n",
    "    iteration_count += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Task 4, run the apriori the user specified increments\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Task 5, run the association rule mining to find best rules\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Task 6, display mined rules."
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}